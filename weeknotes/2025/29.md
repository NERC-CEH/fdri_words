# 2025 Week 29 DRAFT

Completely missed doing last weeks update, so due a double does this week.

## ‚ùáÔ∏è Lots of improvments in the UI

### Scatterplots

<img width="1317" height="664" alt="image" src="https://github.com/user-attachments/assets/75a0c2e2-292d-4ba8-b6c7-da01d4d60302" />

[Click the chart icon on the right to toggle between line charts and scatterplots.](https://dri-ui.staging.eds.ceh.ac.uk/fdri/sites/23B9-FKUR-TW4G?view=explore&startDate=2025-06-23T23%3A00%3A00.000Z&endDate=2025-07-08T23%3A59%3A59.000Z&variables=fdri-one_minute-7ERZ-EEAE-23ZK-WindDir%2Cfdri-one_minute-23B9-FKUR-TW4G-WindDir)


### Cross Network Data Plotting

<img width="1252" height="882" alt="image" src="https://github.com/user-attachments/assets/e66016f3-228a-4890-896e-4d338351f358" />

[Live data from FDRI and COSMOS can be compared in the same chart.](https://dri-ui.staging.eds.ceh.ac.uk/fdri/sites/23B9-FKUR-TW4G?view=explore&startDate=2025-06-30T23%3A00%3A00.000Z&endDate=2025-07-08T23%3A59%3A59.000Z&variables=cosmos-PT30M-PLYNL-TA%2Cfdri-one_minute-23B9-FKUR-TW4G-AirTemp_C)


### Temporary Corrections

<img width="1448" height="975" alt="image" src="https://github.com/user-attachments/assets/5d8d4414-546a-4f0c-8886-f71be5c124ab" />

Since the FDRI data is coming in unprocessed from the sensors and we aren't ready to plot the processed data, we have added a temporary correction box to the explorer chart table, that allows on the fly corrections to happen.
This allows any abitary calculations to be appended to the existing data.

### Controls to print, zoom in/out etc
<img width="1399" height="1180" alt="image" src="https://github.com/user-attachments/assets/5e84dc92-39a8-4d7e-80a3-8cccc6f7faf8" />


We've enabled the plotty controls to allow downloading the charts as png's for reporting and zoom in/out controls.


## ‚ÄºÔ∏è Incident resulting in downtime


<img width="802" height="357" alt="service-unavailable-503" src="https://github.com/user-attachments/assets/37dadcd7-6bbc-4c0c-93a6-0f974d35090b" />


We have a fun incident last week, that was an odd combination of bad luck.
 1. We released a change to our staging environment that was missing some changes to config required for k8s which broke the deployment
 2. Shortly after the [flux](https://fluxcd.io/) Personal Access Token expired
 3. We couldn't rollback the change from 1. since the credentials had expired
 4. The sim cards used for our sensors ran out of data so data stopped coming in at a similar time to above (this was completely unrelated)

The solution was to update the flux personal access token and [document the process of doing so for next time.](https://github.com/NERC-CEH/fdri_words_private/blob/main/k8s/flux.md#rotating-the-flux-personal-access-token)
We could then rollback and get things working, fix and roll out the change üöÄ.

## ‚ûï Connecting Data to Metadata
<img width="768" height="372" alt="image" src="https://github.com/user-attachments/assets/30b21115-b7f1-467d-a218-ffc3833c823c" />


Our Ingestion/API/UI apps for FDRI are currently tracking the site via the campbell cloud id, which is forced into the MQTT topic by campbell's software, this isn't ideal as this id isn't used anywhere else. We are looking at confirming what the site id's should be across FDRI and how to add them to the messages so once we have metadata we can start looking it up.


## ü™£ Should we use a database (maybe not)

<img width="1374" height="485" alt="image" src="https://github.com/user-attachments/assets/4c5020e5-3cea-4de7-a472-cbe377942e5f" />


Continuing our previous discussions on outputing our processed data into a database instead of parquet files on s3, the decision has been reversed (again) so looks like parquet on s3 might be the better fit for now.

We are going to have a lot of sensor data, if we go with a narrow table design; all data in the same db table (time, site_id, variable_id, value). We would have 25,000 rows being added **per minute**.
With a total of 500 loggers x 50 variables per logger x 60 minutes x 24 hours x 365 days = >13 billion "data rows" per year. est 100-500GB per year. Tools like [tigerdata](https://www.tigerdata.com/) would help with this, but especially considering once processed this data is mostly read only, parquet might be a better fit.

Other solutions we discussed were having a wide table for known variables (time, site_id, air_temp, wind_speed, ...)  and a narrow table for unknown variables, but this adds additional complexity. Another solution is having a table per site, but then we need to managed 500 db tables.


Next steps: we have decided to continue with parquet on s3 partitioned by site and day (All variables in the same parquet file), exactly the same as our unprocessed data for now. We will continue assessing and learning as we build.



## üåû Switching to UV

<img width="825" height="378" alt="image" src="https://github.com/user-attachments/assets/1620a3db-3827-4e79-baa4-5b7f8042068c" />

[https://docs.astral.sh/uv/](https://docs.astral.sh/uv/)

Since our work is heavily in progress we set off without lockfiles, which we've had surprisingly few issues from. But it's overdue to start locking our dependencies and why not switch to modern python tooling in the progress.
All our python code is now using uv.


## NRFA data ingestion

We are just in the process of writing out the NRFA data to our level 0 (structured raw data) s3 bucket. 
Our partitioning structure for this will be different to COSMOS and FDRI, something like:
```
/nrfa/dataset=daily-flow/batch=ea_api/site=<nrfa_id>/data.parquet
```

Which will mean querying from the api/ui will be different, the key difference being the batch concept.


