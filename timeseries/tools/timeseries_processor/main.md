# Timeseries Processor
The timeseries processor application is part of the FDRI architecture and handles all the processing between **level 0** and **processed**.

<img src="./images/fdri_architecture.png" alt="FDRI architecture" height="400px">

This documentation will provide a higher level overview of the application. For guidance on how to run the application, and how to contribute, see the repository [README](https://github.com/NERC-CEH/dri-timeseries-processor).

## Table of contents
1. [Introduction](#introduction)
2. [Components](#components)
    1. [Inputs](#inputs)
    2. [Building timeseries IDs to process](#building-timeseries-ids-to-process)
    3. [Corrections](#corrections)
    4. Quality control
    5. Infilling
    6. Aggregation/Derivation
    7. Outputs
3. Flags
4. [Glossary](#glossray)


### Introduction
The timeseries processor handles the processing of timeseries data within the FDRI project and is powered by configurations available through the [metadata store](https://dri-metadata-api.staging.eds.ceh.ac.uk/doc/reference). It takes level 0 data, which has been ingested through the [ingester application](https://github.com/NERC-CEH/dri-ingestion), processes the data based on the configurations, and exports the data into a database.

The tool is a python application which takes a set of arguments (see [Inputs](#inputs)). The processor runs automatically at X every day via a workflow orchestration tool called Argo Workflows which is hosted in the FDRI kubernetes cluster. This is managed in the [kubernetes infrastructure repository](https://github.com/NERC-CEH/dri-infrastructure-k8s-staging). The application can also be run as a one-off job as and when required.


### Components
The application can be split into several components, each of which will be discussed briefly below.

Insert flowchart

---

#### Inputs

The application takes a variety of user inputs entered as command-line arguments. These arguments define the date range and the timeseries IDs to be processed.

> period (**required**):\
The period of time to process
- Must be a valid [ISO8601 duration](https://docs.digi.com/resources/documentation/digidocs/90001488-13/reference/r_iso_8601_duration_format.htm)
- Must not have a time component
- Can be a combination of days, weeks, months and years

> --end_date (**optional**)\
The start date for processing
- Must be of the format YYYY-MM-DD
- If not provided then todays date is used

> --sites (**optional**)\
The site(s) to process
- If empty then all sites will be built
- Entered sites are checked against available sites in the metadata store and removed if not found
- sites must be seperated by a comma, be 5 characters long and not contain special characters
- sites can be lower or upper case

> --columns (**optional**)\
The column(s) to process
- If empty then all columns will be built
- Columns must be seperated by a comma (no spaces)
- Columns can be upper or lower case

> --periodicity (**optional**)\
The period(s) to process
- Must be a valid [ISO8601 duration](https://docs.digi.com/resources/documentation/digidocs/90001488-13/reference/r_iso_8601_duration_format.htm)
- Multiple periodicities must be seperated by a comma
- Can be upper or lower case

Some examples:

Get the last two days data for all sites, columns and periodicities

```
python -m dritimeseriesprocessor P2D
```

Get the last two days data from 2024-03-05 for all sites, columns and periodicities

```
python -m dritimeseriesprocessor P2D --end_date=2024-03-05
```

Get the last months data from 2024-03-05 for ALCI and BUNNY sites and all columns and periodicities

```
python -m dritimeseriesprocessor P1M --end_date=2024-03-05 --sites=alic1,bunny
```

Get the last two days data from 2024-03-05 for ALCI and BUNNY sites, variables TA and PA and all periodicities

```
python -m dritimeseriesprocessor P2D --end_date=2024-03-05 --sites=alic1,bunny --columns=TA,PA
```

Get the last two days data from 2024-03-05 for ALCI and BUNNY sites, variables TA and PA and a periodicity of 30 mins

```
python -m dritimeseriesprocessor P2D --end_date=2024-03-05 --sites=alic1,bunny --columns=TA,PA --periodicity=PT30M
```

---

#### Building Timeseries IDs to process

The arguments above are used to build the timeseries IDs to process and attach any other required metadata. Using the example below we can follow this process.

```
python -m dritimeseriesprocessor P2D --sites=eastb --columns=WD --periodicity=PT30M
```

The first stage is to collect the timeseries IDs based on the arguments. These are generated by sending the arguments to the [datasets](https://dri-metadata-api.staging.eds.ceh.ac.uk/id/dataset?_limit=10) endpoint in the metadata store. This gives a dictionary of timeseries IDs along with some metadata.

```
{
    "http://fdri.ceh.ac.uk/id/dataset/cosmos-eastb-wd_30min_processed":
    {
        "ts_def": "http://fdri.ceh.ac.uk/ref/cosmos/time-series/wd_30min_processed",
        "resolution": "PT30M",
        "periodicity": "PT30M",
        "processing_level": "processed",
        "sourceBucket": "ukceh-fdri-staging-timeseries-qc",
        "sourceDataset": "PROCESSED_DATA_30MIN",
        "sourceColumnName": "WD",
        "sourceSite": "EASTB"
    }
}
```

Each timeseries ID might have some other timeseries IDs that it is dependent on to be processed. There are two types of dependency:

Each timeseries ID might have certain processing dependencies. These are generated by calling the [data processing configuration](https://dri-metadata-api.staging.eds.ceh.ac.uk/id/data-processing-configuration?_limit=10) endpoint in the metadata store.

Each timeseries ID might be dependant on other timeseries IDs (for example, a `processed` timeseries will require the associated `raw` timeseries). These dependencies are stored with the timeseries definition, rather than the timeseries ID (see [glossary](#glossary)) and are generated by calling the [timeseries definition](https://dri-metadata-api.staging.eds.ceh.ac.uk/ref/time-series-definition?_limit=10) endpoint in the metadata store.

Any dependent timeseries IDs are added to the dictionary.

```
{
    "http://fdri.ceh.ac.uk/id/dataset/cosmos-eastb-wd_30min_processed":
    {
        "ts_def": "http://fdri.ceh.ac.uk/ref/cosmos/time-series/wd_30min_processed",
        "resolution": "PT30M",
        "periodicity": "PT30M",
        "processing_level": "processed",
        "sourceBucket": "ukceh-fdri-staging-timeseries-qc",
        "sourceDataset": "PROCESSED_DATA_30MIN",
        "sourceColumnName": "WD",
        "sourceSite": "EASTB"
    },
    "http://fdri.ceh.ac.uk/id/dataset/cosmos-eastb-scans_30min_raw":
    {
        "ts_def": "http://fdri.ceh.ac.uk/ref/cosmos/time-series/scans_30min_raw",
        "resolution": "PT30M",
        "periodicity": "PT30M",
        "processing_level": "raw",
        "sourceBucket": "ukceh-fdri-staging-timeseries-level-0",
        "sourceDataset": "LIVE_SOILMET_30MIN",
        "sourceColumnName": "SCANS",
        "sourceSite": "EASTB"
    },
    "http://fdri.ceh.ac.uk/id/dataset/cosmos-eastb-battv_30min_raw":
    {
        "ts_def": "http://fdri.ceh.ac.uk/ref/cosmos/time-series/battv_30min_raw",
        "resolution": "PT30M",
        "periodicity": "PT30M",
        "processing_level": "raw",
        "sourceBucket": "ukceh-fdri-staging-timeseries-level-0",
        "sourceDataset": "LIVE_SOILMET_30MIN",
        "sourceColumnName": "BATTV",
        "sourceSite": "EASTB"
    },
    "http://fdri.ceh.ac.uk/id/dataset/cosmos-eastb-wd_30min_raw":
    {
        "ts_def": "http://fdri.ceh.ac.uk/ref/cosmos/time-series/wd_30min_raw",
        "resolution": "PT30M",
        "periodicity": "PT30M",
        "processing_level": "raw",
        "sourceBucket": "ukceh-fdri-staging-timeseries-level-0",
        "sourceDataset": "LIVE_SOILMET_30MIN",
        "sourceColumnName": "WD",
        "sourceSite": "EASTB"
    }
}
```

Finally, some additional metadata (`method_type` and `inputs`) are added from the timeseries definition response and a `load` parameter is calculated indicated whether the data needs to be loaded (i.e. `raw` timeseries required for `processed` timeseries). For the first entry in the dictionary above this gives us the following structure.

```
{
    "http://fdri.ceh.ac.uk/id/dataset/cosmos-eastb-wd_30min_processed":
    {
        "ts_def": "http://fdri.ceh.ac.uk/ref/cosmos/time-series/wd_30min_processed",
        "resolution": "PT30M",
        "periodicity": "PT30M",
        "processing_level": "processed",
        "sourceBucket": "ukceh-fdri-staging-timeseries-qc",
        "sourceDataset": "PROCESSED_DATA_30MIN",
        "sourceColumnName": "WD",
        "sourceSite": "EASTB",
        "method_type": "process",
        "inputs": ["http://fdri.ceh.ac.uk/ref/cosmos/time-series/wd_30min_raw"],
        "load": False
    }
}

All the metadata required to process the requested timeseries IDs has now been collected and the processing can begin.

#### Corrections





### Glossary

**Level 0**:\
**Timeseries ID**:\
**Timeseries definition**: